# AIBYHER: Bias Mitigation in Language Models

A comprehensive framework for measuring, analyzing, and mitigating bias in large language models through counterfactual data generation, filtering pipelines, and fine-tuning with LoRA (Low-Rank Adaptation).

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Detailed Usage](#detailed-usage)
  - [1. Dataset Generation](#1-dataset-generation)
  - [2. Counterfactual Generation](#2-counterfactual-generation)
  - [3. Filtering Pipeline](#3-filtering-pipeline)
  - [4. Model Training](#4-model-training)
  - [5. Inference](#5-inference)
  - [6. Evaluation](#6-evaluation)
- [Pipeline Architecture](#pipeline-architecture)
- [Configuration](#configuration)
- [Evaluation Metrics](#evaluation-metrics)
- [Ablation Studies](#ablation-studies)
- [Testing](#testing)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)

## Overview

AIBYHER is a research framework designed to address algorithmic bias in language models across sensitive attributes such as gender, religion, and nationality. The project implements a multi-stage pipeline that:

1. **Generates** synthetic datasets across multiple domains (hiring, healthcare, education, finance, moderation)
2. **Creates** counterfactual examples by systematically varying sensitive attributes
3. **Filters** low-quality and biased responses using semantic invariance checks
4. **Fine-tunes** language models using parameter-efficient LoRA training
5. **Evaluates** models for both bias metrics and task capability preservation

This approach enables researchers and practitioners to build fairer AI systems while maintaining model performance.

## Key Features

### ğŸ¯ Multi-Domain Bias Testing
- **5 Critical Domains**: Hiring, Healthcare, Education, Finance, Content Moderation
- **Intersectional Analysis**: Examines bias across multiple demographic attributes simultaneously
- **Real-World Scenarios**: Templates based on actual decision-making contexts

### ğŸ”„ Counterfactual Data Generation
- Systematic variation of sensitive attributes (gender, religion, nationality)
- Maintains semantic consistency across counterfactual pairs
- Preserves task context while changing demographic information

### ğŸ§¹ Intelligent Filtering Pipeline
- **Heuristic Filters**: Removes profanity, slurs, and harmful content
- **Semantic Invariance**: Ensures response consistency across counterfactuals (85% similarity threshold)
- **Sentiment Alignment**: Filters responses with divergent sentiment (Â±0.2 tolerance)
- **Refusal Detection**: Identifies and handles model refusal responses

### ğŸš€ Efficient Fine-Tuning
- **LoRA Integration**: Parameter-efficient training using Low-Rank Adaptation
- **Base Model**: Qwen 1.5-1.8B-Chat (customizable)
- **Optimized Configuration**: bfloat16 precision, gradient checkpointing
- **Resource Efficient**: Trains on consumer hardware

### ğŸ“Š Comprehensive Evaluation
- **Bias Metrics**: Sentiment disparity, refusal rate analysis, intersectional group comparisons
- **Capability Metrics**: Task performance preservation checks
- **Visualization**: Automated plotting of bias-capability tradeoffs

## Project Structure

```
aibyher/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ attributes.yaml              # Sensitive attribute definitions
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ hiring.txt                   # Hiring decision templates
â”‚   â”œâ”€â”€ health.txt                   # Healthcare recommendation templates
â”‚   â”œâ”€â”€ education.txt                # Educational assessment templates
â”‚   â”œâ”€â”€ finance.txt                  # Financial advisory templates
â”‚   â””â”€â”€ moderation.txt               # Content moderation templates
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_dataset.py          # Initial dataset generation
â”‚   â”œâ”€â”€ generate_counterfactuals.py  # Counterfactual pair creation
â”‚   â”œâ”€â”€ filter_pipeline.py           # Quality and invariance filtering
â”‚   â”œâ”€â”€ utils.py                     # Shared utilities (sentiment, refusal detection)
â”‚   â”œâ”€â”€ sweep_runner.py              # Hyperparameter sweep orchestration
â”‚   â””â”€â”€ evaluate_sweep.py            # Sweep results analysis
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ lora_train.py                # LoRA fine-tuning script
â”‚   â””â”€â”€ config_lora.json             # Training hyperparameters
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ bias_metrics.py              # Bias measurement and analysis
â”‚   â”œâ”€â”€ capability_metrics.py        # Task performance evaluation
â”‚   â””â”€â”€ plot_bias_tradeoffs.py       # Visualization utilities
â”œâ”€â”€ ablations/
â”‚   â”œâ”€â”€ attribute_removal.py         # Attribute ablation experiments
â”‚   â””â”€â”€ filter_sensitivity.py        # Filter threshold analysis
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_filter_pipeline.py      # Pipeline unit tests
â”‚   â”œâ”€â”€ test_metrics.py              # Metrics calculation tests
â”‚   â””â”€â”€ test_utils.py                # Utility function tests
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory.ipynb            # Data exploration and analysis
â”œâ”€â”€ inference.py                     # Model inference script
â””â”€â”€ pytest.ini                       # Test configuration
```

### Directory Purposes

- **`configs/`**: YAML configurations for sensitive attributes and domain settings
- **`templates/`**: Prompt templates for each task domain
- **`scripts/`**: Core pipeline scripts for data generation and processing
- **`training/`**: Model fine-tuning code and configurations
- **`evaluation/`**: Bias and capability evaluation metrics
- **`ablations/`**: Experimental scripts for sensitivity analysis
- **`tests/`**: Unit tests for pipeline components
- **`notebooks/`**: Jupyter notebooks for exploratory analysis

## Installation

### Prerequisites

- Python 3.8+
- CUDA 11.8+ (for GPU training)
- 16GB+ RAM recommended
- 10GB+ disk space for models and data

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/theparidhisharma/AIBYHER.git
cd AIBYHER
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets peft accelerate
pip install sentence-transformers sentencepiece
pip install matplotlib pandas numpy
pip install pytest pytest-cov
```

4. **Verify installation**
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

## Quick Start

Run the complete pipeline end-to-end:

```bash
# 1. Generate initial dataset
python scripts/generate_dataset.py

# 2. Generate counterfactual pairs (if implemented)
python scripts/generate_counterfactuals.py

# 3. Filter dataset for quality
python scripts/filter_pipeline.py

# 4. Train the model
python training/lora_train.py

# 5. Run inference
python inference.py

# 6. Evaluate results
python evaluation/bias_metrics.py
python evaluation/capability_metrics.py
python evaluation/plot_bias_tradeoffs.py
```

## Detailed Usage

### 1. Dataset Generation

The `generate_dataset.py` script creates synthetic examples across five domains.

**Configuration:**
- **Domains**: hiring, health, moderation, education, finance
- **Samples per domain**: 1,000 (configurable via `SAMPLES_PER_DOMAIN`)
- **Output**: `data/raw/initial_dataset.jsonl`

**Usage:**
```bash
python scripts/generate_dataset.py
```

**Example Output Format:**
```json
{
  "id": "uuid-here",
  "task_domain": "hiring",
  "task_type": "generation",
  "prompt": "Evaluate candidate with background in...",
  "response": "<TO_BE_GENERATED>",
  "sensitive_attributes": {
    "gender": "female",
    "religion": "Muslim",
    "nationality": "Indian"
  },
  "counterfactual_group": "cf_00001",
  "counterfactual_axis": null,
  "source": "synthetic",
  "generation_seed": 1234567,
  "filtering_metadata": null
}
```

**Customization:**

Modify `SAMPLES_PER_DOMAIN` in the script:
```python
SAMPLES_PER_DOMAIN = 2000  # Generate 2000 samples per domain
```

Add new domains by extending `DOMAIN_TEMPLATES`:
```python
DOMAIN_TEMPLATES = {
    "hiring": hiring_template,
    # ... existing domains
    "legal": legal_template  # Add new domain
}
```

### 2. Counterfactual Generation

Creates matched pairs where only sensitive attributes vary.

**Purpose:**
- Systematic variation of demographic attributes
- Maintains semantic equivalence
- Enables controlled bias measurement

**Note:** The `generate_counterfactuals.py` script is a placeholder in the current codebase. Implementation should:
1. Group examples by task context
2. Generate variants for each sensitive attribute
3. Assign counterfactual group IDs
4. Preserve non-sensitive aspects

**Expected Output**: `data/counterfactuals/merged.jsonl`

### 3. Filtering Pipeline

The filtering pipeline ensures data quality and removes biased examples.

**Components:**

#### Heuristic Filtering
Removes explicit harmful content:
```python
BLOCKLIST = [
    r"\bfuck\b", r"\bnigger\b", r"\bfag\b", r"\bkill yourself\b"
]
```

#### Semantic Invariance Check
Ensures counterfactual responses are similar:
- **Embedding similarity**: â‰¥85% cosine similarity (using `all-MiniLM-L6-v2`)
- **Sentiment alignment**: Â±0.2 tolerance
- **Refusal consistency**: Same refusal/compliance behavior

**Usage:**
```bash
python scripts/filter_pipeline.py
```

**Configuration:**
```python
# Adjust thresholds in filter_pipeline.py
SIMILARITY_THRESHOLD = 0.85    # Cosine similarity threshold
SENTIMENT_TOLERANCE = 0.2      # Maximum sentiment difference
```

**Output:**
- Filtered dataset: `data/filtered/filtered_dataset.jsonl`
- Console log: Number of samples kept/removed

**Example Output:**
```
Filtered down to 3247 samples from 5000.
```

### 4. Model Training

Fine-tune language models using LoRA for parameter-efficient adaptation.

**Base Configuration** (`training/config_lora.json`):
```json
{
  "base_model": "Qwen/Qwen1.5-1.8B-Chat",
  "output_dir": "outputs/lora_qwen_finetuned",
  "learning_rate": 2e-4,
  "batch_size": 4,
  "epochs": 3,
  "max_seq_len": 512,
  "target_modules": ["q_proj", "v_proj"],
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "fp16": true,
  "save_steps": 200,
  "logging_steps": 10
}
```

**Usage:**
```bash
python training/lora_train.py
```

**Training Process:**
1. Loads base model (Qwen 1.5-1.8B-Chat)
2. Applies LoRA adapters to attention layers
3. Trains on filtered dataset
4. Saves checkpoints every 200 steps
5. Final model saved to `outputs/lora_qwen_finetuned/`

**Customization:**

Switch to larger models:
```json
{
  "base_model": "Qwen/Qwen1.5-7B-Chat",
  "batch_size": 2,  // Reduce for larger models
  "lora_r": 32      // Increase for more capacity
}
```

Adjust training duration:
```json
{
  "epochs": 5,
  "save_steps": 500
}
```

**Memory Requirements:**
- 1.8B model: ~8GB GPU RAM
- 7B model: ~20GB GPU RAM (with LoRA + fp16)

### 5. Inference

Generate responses from the fine-tuned model.

**Usage:**
```bash
python inference.py
```

**Configuration:**
```python
MODEL_DIR = "outputs/lora_qwen_finetuned"
INPUT_FILE = Path("data/filtered/filtered_dataset.jsonl")
OUTPUT_FILE = Path("data/filtered/generated_responses.jsonl")
```

**Generation Parameters:**
```python
output = model.generate(
    **inputs, 
    max_new_tokens=128,      # Maximum response length
    do_sample=True,          # Enable sampling
    top_p=0.95,              # Nucleus sampling threshold
    temperature=0.7          # Sampling temperature
)
```

**Output Format:**
Each input sample is augmented with a `response` field:
```json
{
  "prompt": "...",
  "response": "Generated text here",
  "sensitive_attributes": {...}
}
```

### 6. Evaluation

#### Bias Metrics

Measures fairness across demographic groups.

**Usage:**
```bash
python evaluation/bias_metrics.py
```

**Metrics Computed:**

1. **Intersectional Group Analysis**
   - Average sentiment per demographic combination
   - Refusal rate per group
   - Sample counts

2. **Attribute-Level Disparities**
   - Per-attribute sentiment averages
   - Per-attribute refusal rates
   - Distribution statistics

**Output**: `data/logs/bias_metrics.json`

**Example Output:**
```json
{
  "intersectional_groups": {
    "female_Muslim_Indian": {
      "avg_sentiment": 0.6234,
      "refusal_rate": 0.0450,
      "count": 234
    },
    "male_Christian_American": {
      "avg_sentiment": 0.6189,
      "refusal_rate": 0.0423,
      "count": 242
    }
  },
  "attribute_disparities": {
    "gender": {
      "female": {"avg_sentiment": 0.6210, "refusal_rate": 0.0441},
      "male": {"avg_sentiment": 0.6203, "refusal_rate": 0.0438}
    }
  }
}
```

#### Capability Metrics

Ensures the model retains task performance.

**Usage:**
```bash
python evaluation/capability_metrics.py
```

**Test Cases:**
- Basic question-answering tasks
- Factual knowledge retrieval
- Simple reasoning

**Output**: `data/logs/capability_metrics.json`

**Example Output:**
```json
{
  "task_count": 3,
  "correct": 3,
  "accuracy": 1.0
}
```

#### Visualization

Generate plots of bias-capability tradeoffs.

**Usage:**
```bash
python evaluation/plot_bias_tradeoffs.py
```

**Output**: 
- `data/logs/group_bias_metrics.png`
- Horizontal bar charts showing:
  - Average sentiment per group
  - Refusal rate per group

## Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIBYHER Pipeline Flow                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DATA GENERATION
   â”œâ”€â”€ Templates (hiring, health, education, finance, moderation)
   â”œâ”€â”€ Attribute Sampling (gender, religion, nationality)
   â””â”€â”€ Output: data/raw/initial_dataset.jsonl
                    â†“
2. COUNTERFACTUAL CREATION
   â”œâ”€â”€ Group by task context
   â”œâ”€â”€ Vary sensitive attributes
   â””â”€â”€ Output: data/counterfactuals/merged.jsonl
                    â†“
3. FILTERING PIPELINE
   â”œâ”€â”€ Heuristic Filter (profanity, slurs)
   â”œâ”€â”€ Semantic Invariance (85% similarity)
   â”œâ”€â”€ Sentiment Alignment (Â±0.2 tolerance)
   â””â”€â”€ Output: data/filtered/filtered_dataset.jsonl
                    â†“
4. MODEL TRAINING
   â”œâ”€â”€ LoRA Fine-tuning (Qwen 1.5-1.8B)
   â”œâ”€â”€ Instruction Format
   â””â”€â”€ Output: outputs/lora_qwen_finetuned/
                    â†“
5. INFERENCE
   â”œâ”€â”€ Generate responses
   â””â”€â”€ Output: data/filtered/generated_responses.jsonl
                    â†“
6. EVALUATION
   â”œâ”€â”€ Bias Metrics (sentiment, refusal)
   â”œâ”€â”€ Capability Metrics (QA accuracy)
   â””â”€â”€ Visualization (plots)
```

## Configuration

### Attribute Configuration

Define sensitive attributes in `configs/attributes.yaml`:

```yaml
gender:
  - male
  - female
  - non-binary

religion:
  - Christian
  - Muslim
  - Hindu
  - Buddhist
  - Jewish
  - Atheist

nationality:
  - American
  - Indian
  - Chinese
  - British
  - Brazilian
```

### Training Configuration

Modify `training/config_lora.json`:

```json
{
  "base_model": "Qwen/Qwen1.5-1.8B-Chat",
  "output_dir": "outputs/lora_qwen_finetuned",
  "learning_rate": 2e-4,
  "batch_size": 4,
  "epochs": 3,
  "max_seq_len": 512,
  "target_modules": ["q_proj", "v_proj"],
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "report_to": "none",
  "fp16": true,
  "save_total_limit": 1,
  "save_steps": 200,
  "logging_steps": 10
}
```

**Key Parameters:**
- `lora_r`: LoRA rank (higher = more capacity, slower training)
- `lora_alpha`: Scaling factor (typically 2x lora_r)
- `target_modules`: Which attention layers to adapt
- `batch_size`: Samples per training step
- `max_seq_len`: Maximum sequence length

## Evaluation Metrics

### Bias Metrics

1. **Sentiment Disparity**
   - Measures average sentiment differences across groups
   - Uses DistilBERT for sentiment scoring
   - Range: [0, 1] where 1 = most positive

2. **Refusal Rate**
   - Percentage of responses containing refusal language
   - Detects: "I cannot", "I'm sorry", "as an AI"
   - Lower disparity across groups indicates less bias

3. **Intersectional Analysis**
   - Examines combined effect of multiple attributes
   - Identifies which combinations face discrimination
   - Provides group-level statistics

### Capability Metrics

1. **Question-Answering Accuracy**
   - Tests factual knowledge retention
   - Uses RoBERTa-base SQuAD2 model
   - Baseline: 100% on simple QA tasks

2. **Task-Specific Performance**
   - Domain accuracy (hiring, health, etc.)
   - Response coherence
   - Instruction following

### Fairness Indicators

- **Max Sentiment Gap**: Maximum difference in sentiment across groups
- **Refusal Rate Variance**: Standard deviation of refusal rates
- **Intersectional Parity**: Ratio of max to min group performance

## Ablation Studies

### Attribute Removal

Test bias impact of individual attributes:

```bash
python ablations/attribute_removal.py
```

**Experiments:**
- Remove gender â†’ measure bias change
- Remove religion â†’ measure bias change
- Remove nationality â†’ measure bias change

### Filter Sensitivity

Analyze filter threshold impact:

```bash
python ablations/filter_sensitivity.py
```

**Tests:**
- Similarity thresholds: [0.75, 0.80, 0.85, 0.90, 0.95]
- Sentiment tolerances: [0.1, 0.2, 0.3, 0.4]
- Dataset size vs. quality tradeoff

## Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test file
pytest tests/test_filter_pipeline.py -v

# Run with verbose output
pytest -v
```

**Test Coverage:**
- Filter pipeline logic
- Metric calculations
- Utility functions (sentiment, refusal detection)
- Data processing workflows

## Common Issues & Troubleshooting

### CUDA Out of Memory

**Solution 1**: Reduce batch size
```json
{
  "batch_size": 2  // or 1
}
```

**Solution 2**: Use gradient accumulation
```json
{
  "gradient_accumulation_steps": 4
}
```

**Solution 3**: Enable gradient checkpointing (already enabled)

### Slow Training

**Solution 1**: Use mixed precision training (already enabled with fp16)

**Solution 2**: Increase batch size if memory allows
```json
{
  "batch_size": 8
}
```

**Solution 3**: Reduce sequence length
```json
{
  "max_seq_len": 256
}
```

### Low Filtering Yield

If too many samples are filtered out:

1. **Relax similarity threshold**:
```python
SIMILARITY_THRESHOLD = 0.80  # from 0.85
```

2. **Increase sentiment tolerance**:
```python
SENTIMENT_TOLERANCE = 0.3  # from 0.2
```

3. **Review counterfactual generation** to ensure quality

### Dependencies Issues

**Install transformers from source**:
```bash
pip install git+https://github.com/huggingface/transformers
```

**Force reinstall**:
```bash
pip install --force-reinstall transformers peft
```

## Advanced Usage

### Custom Domains

Add new task domains:

1. **Create template** (`templates/legal.txt`):
```
Analyze the legal case involving {{name}}, a {{nationality}} {{gender}} attorney...
```

2. **Update domain list** (`scripts/generate_dataset.py`):
```python
DOMAIN_TEMPLATES = {
    # ... existing domains
    "legal": legal_template
}
```

3. **Regenerate dataset**:
```bash
python scripts/generate_dataset.py
```

### Hyperparameter Sweeps

Run parameter grid search:

```bash
python scripts/sweep_runner.py
```

**Configure sweep** in the script:
```python
SWEEP_CONFIG = {
    "lora_r": [8, 16, 32],
    "learning_rate": [1e-4, 2e-4, 5e-4],
    "batch_size": [2, 4, 8]
}
```

**Analyze results**:
```bash
python scripts/evaluate_sweep.py
```

### Custom Base Models

Switch to different language models:

```json
{
  "base_model": "meta-llama/Llama-2-7b-chat-hf",
  // Adjust target_modules for Llama architecture
  "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
}
```

### Multi-GPU Training

Enable distributed training:

```bash
accelerate config  # Configure multi-GPU settings
accelerate launch training/lora_train.py
```

## Performance Benchmarks

**Dataset Generation**: ~5 minutes for 5,000 samples  
**Filtering**: ~15 minutes for 5,000 samples  
**Training (1.8B model)**: ~2 hours for 3 epochs on A100  
**Inference**: ~500 samples/minute on A100  
**Evaluation**: ~5 minutes for full metrics  

## Data Privacy & Ethics

This framework handles sensitive demographic information. Ensure:

1. **Data Anonymization**: Remove personally identifiable information
2. **Responsible Use**: Use synthetic data for research purposes
3. **Bias Awareness**: Metrics are exploratory, not definitive
4. **Human Review**: Always validate results with domain experts
5. **Compliance**: Follow institutional review board (IRB) guidelines

## Roadmap

- [ ] Implement full counterfactual generation logic
- [ ] Add more task domains (legal, customer service)
- [ ] Expand attribute coverage (age, disability, socioeconomic status)
- [ ] Multi-language support
- [ ] Interactive bias exploration dashboard
- [ ] Integration with fairness auditing frameworks
- [ ] Production deployment guide

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

**Development Setup:**
```bash
pip install -e ".[dev]"
pre-commit install
```

**Code Style:**
- Follow PEP 8
- Use type hints
- Add docstrings to functions
- Write unit tests for new features

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{aibyher2024,
  author = {Sharma, Paridhi},
  title = {AIBYHER: Bias Mitigation in Language Models},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/theparidhisharma/AIBYHER}
}
```

## Acknowledgments

- **Qwen Team** for the base language model
- **Hugging Face** for transformers and PEFT libraries
- **Sentence Transformers** for semantic similarity models
- Open-source AI safety research community

## License

This project is licensed under the MIT License. See `LICENSE` file for details.

## Contact

**Paridhi Sharma**  
GitHub: [@theparidhisharma](https://github.com/theparidhisharma)

For questions, issues, or collaboration opportunities, please open an issue on GitHub.

---

**Built with â¤ï¸ for fairer AI systems**
